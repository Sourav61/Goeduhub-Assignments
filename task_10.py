# -*- coding: utf-8 -*-
"""Task-10.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/Sourav61/Goeduhub-Assignments/blob/main/Task_10.ipynb
"""

from google.colab import drive
drive.mount('/content/drive')

"""Author: <a href = "https://github.com/Sourav61">Sourav Pahwa</a>
<br>ID: GO_STP_13420
"""

#@title <b>Discuss the concept of One-Hot-Encoding, Multicollinearity and the Dummy Variable Trap.</b>  

print("""1. One Hot Encoding 

  * It is one of the most popular and widely used algorithms to convert the categorical data into numerical data.

  * OneHotEncoder(OHE) can be used for both nominal and ordinal data imported from sklearn.preprocessing.

  * It can be easily performed using Label Encoder and One Hot Encoder imported from sklearn.preprocessing. 

2. Multicollinearity

  * The Dummy Variable Trap leads to the problem known as Multicollinearity. It occurs where there is a dependency between the independent features.

  * Multicollinearity is the occurrence of high intercorrelations among two or more independent variables

  * In general, multicollinearity can lead to wider confidence intervals that produce less reliable probabilities in terms of the effect of independent variables in a model.

  * So, in order to overcome the problem of multicollinearity, one of the dummy variables has to
be dropped.

3. Dummy Variable Trap

  * The Dummy Variable Trap leads to the problem known as Multicollinearity. Multicollinearity occurs where there is a dependency between the independent features.

  * The Dummy Variable trap is a scenario in which the independent variables are multicollinear - a scenario in which two or more variables are highly correlated.
  
  * In general, multicollinearity can lead to wider confidence intervals that produce less reliable probabilities in terms of the effect of independent variables in a model.

  * So, in order to overcome the problem of multicollinearity, one of the dummy variables has to
be dropped.""")

#@title <b>What is Nominal and Ordinal Variables?</b>

print("""1. Nominal Data

  * Nominal data is a type of data where order of data doesn't matter i.e. we don't need to sort the data.

  * It can be encoded using get_dummies from pandas library.

  * Student Pass or Fail Classification can be a good example for Nominal Data.

2. Ordinal Data

  * Ordinal Data is a type of data in which order matters i.e. we need to sort the data before performing any operations on it.

  * It can be encoded using get_dummies from pandas library or Label Encoder and One Hot Encoder imported from sklearn.preprocessing.
  
  * Movie Ratings is a good example of Ordinal Data.
  
  """)

"""<b>Salary Dataset of 52 professors having categorical columns. Apply dummy variables concept and one-hot-encoding on categorical columns.</b>  Dataset Link- <a href="https://data.princeton.edu/wws509/datasets/salary.dat">Click here</a>


"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import missingno as msno
from sklearn.preprocessing import OneHotEncoder
from sklearn.preprocessing import LabelEncoder
from sklearn.compose import ColumnTransformer

df = pd.read_table("salary.dat.txt",delim_whitespace=True)
df

df.head(10)

df.tail(10)

df.info()

df.describe(include="all")

df.kurt()

df.skew()

df.keys()

df.columns

df.axes

df.boxplot(rot=45)
plt.show()

df.hist(figsize=(15,20),xrot=45,yrot=45)
plt.show()

df.dtypes

df.duplicated().any()

df.duplicated().sum()

df.isna()

df.isnull().any()

df.isnull().sum()

msno.bar(df.sample(51),color="cyan")
plt.show()

msno.matrix(df.sample(51),color=(1, 0, 1))
plt.show()

df.corr()

fig = plt.figure(figsize = (12,10))
sns.heatmap(df.corr(), cmap='inferno', annot = True) 
plt.show()

corr = df.corr()
sns.heatmap((corr),
xticklabels=corr.columns.values,
yticklabels=corr.columns.values,cmap='cubehelix_r',annot=False,fmt=".2g")
plt.title('Heatmap of Correlation Matrix', fontsize=20)
corr

plt.figure(figsize=(14,14))
sns.heatmap(df.cov(), annot=True, fmt =".2f",square=True,cmap='rainbow')
plt.title("Covariation",fontsize = 15)
plt.show()

df1 = pd.get_dummies(df,columns = ['sx','rk','dg'])
df1.head(10)

df1.tail(10)

df1=df1.drop(['sx_female','rk_associate','dg_masters'], axis=1)
df1.head(10)

df1.tail(10)

le = LabelEncoder()

df2 = df1

df2.sx_male=le.fit_transform(df2.sx_male)
df2.sx_male.head()

df2 = df1

df2.rk_assistant=le.fit_transform(df2.rk_assistant)
df2.rk_assistant.head()

df2 = df1

df2.dg_doctorate=le.fit_transform(df2.dg_doctorate)
df2.dg_doctorate.head()

t = [('sx',OneHotEncoder(),[3]),('rk',OneHotEncoder(),[4]),('dg',OneHotEncoder(),[6])]

ct = ColumnTransformer(transformers=t,remainder="passthrough")
ct

newct=ct.fit_transform(df1) 
newct

ct=ColumnTransformer([('sx',OneHotEncoder(),[0])],remainder="passthrough")
newct=ct.fit_transform(df) 
newct

ct=ColumnTransformer([('rk',OneHotEncoder(),[1])],remainder="passthrough")
newct=ct.fit_transform(df) 
newct

ct=ColumnTransformer([('dg',OneHotEncoder(),[3])],remainder="passthrough")
newct=ct.fit_transform(df) 
newct